\section{Model Order Reduction on Solutions}
The underlying idea of MOR is that each high-dimensional solution can be represented in a low-fidelity space through a reduced number of global basis functions because the parametric dependence has a lower intrinsic dimension. By extracting the most significant features from high-dimensional datasets, MOR enables the creation of reduced-order models that retain essential dynamical behaviour on a reduced basis space. This approach proves particularly advantageous in scenarios where computational resources are limited or rapid simulations are required. 

The MOR procedure usually comprises 2 stages. The first stage is performed offline, which is more expensive and computationally demanding. It involves forming the solution snapshot matrix $\mathbf{S}$ that includes $N$ high-fidelity solutions, each of $M$ degrees of freedom, solved by the classical FEM. The reduced basis space $\mathbf{V}_{rb}$ is then found through specific MOR techniques (i.e. the reduced basis functions that form the space). There are many techniques for generating the reduced basis spaces, such as the Proper Orthogonal Decomposition (POD), the Proper Generalized Decomposition (PGD) and the Principle Component Analysis(PCA). In this project, we focused on using POD on the snapshot matrix to obtain the reduced basis space, and the Python RBniCSx package provided full support for the related operations in POD. 

This project also incorporates the training of the ANN model in the offline stage. During this stage, we generate training parameters and project them onto the reduced basis space to obtain the training outputs for the ANN. We then train the ANN model for predicting solutions on the reduced space. More information on the ANN model will be covered in the next section. 

The second stage is an online stage where we can make use of the trained ANN-POD model and obtain the high-fidelity solution by passing the input parameter $\bm{\mu}$ into the ANN followed by reconstructing the full-order solution with the reduced basis functions obtained from stage one. This approach would significantly reduce our reliance on the costly FEM. 
This online process can be integrated into a computer system with restricted computational resources and memory capacity, enabling quick real-time access to the response of a complex system \cite{hesthaven2015certified}. 

Consider that we now have the reduced basis space $\mathbf{V}_{rb}$ that is used to represent the original function space $\mathbf{V}$. $\mathbf{V}_{rb}$ is formed by the set of $L$ basis functions $\mathbf{u}_{rb} = [\mathbf{u}_1,...,\mathbf{u}_L]$, which represent the $N$ solutions from the snapshot matrix $\mathbf{S}$. Usually, we have $L \ll M$, where $M$ is the original degrees of freedom on the full-order solution so that MOR works well in dimensionality reduction. Each original solution $\mathbf{s}_n$ from $\mathbf{S}$ can be represented on the reduced basis space with equation \ref{eqn: MOR_projection}. 

\begin{equation}
    \sum_{l=1}^{L} \langle \mathbf{s}_n, \mathbf{u}_l \rangle_{\mathbb{R}^M} \mathbf{u}_l
    \label{eqn: MOR_projection}
\end{equation}

Where $\langle \cdot, \cdot \rangle$ defines the inner production action. Hence, to obtain the best set of the reduced basis functions $\mathbf{u_{rb}}$, our goal would be to minimise the error in equation \ref{eqn:MOR_error}:

\begin{equation}
   \epsilon = \sum_{n=1}^{N} \left\|\mathbf{s}_n -  \sum_{l=1}^{L} \langle \mathbf{s}_n, \mathbf{u}_l \rangle_{\mathbb{R}^M} \mathbf{u}_l \right\|_{\mathbb{R}^M} ^2
   \label{eqn:MOR_error}
\end{equation}

\subsection{Proper Orthogonal Decomposition}

The appeal of POD lies in its ability to identify the dominant modes of variation in data through singular value decomposition (SVD). The Schmidt-Eckart-Young theorem for matrices states that the optimal rank-$l$ approximation of a matrix in the Euclidean topology is obtained by retaining the first $l$ terms from the SVD of that matrix \cite{vannieuwenhoven_nicaise_vandebril_meerbergen_2014, Eckart1936TheAO}. These corresponding left eigenvectors form the reduced basis space, where the full-order solutions are projected.  

Consider a snapshot matrix $\mathbf{S}$ of dimension $N \times M$, where $N$ is the number of snapshots and $M$ is the degrees of freedom for each high-fidelity FEM solution. By performing singular value decomposition (SVD) on $\mathbf{S}$, we can express $\mathbf{S}$ as:

\begin{equation}
    \mathbf{S} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^T
    \label{eqn:SVD}
\end{equation}

Where $\mathbf{U}$ is an $N \times N$ matrix containing the left eigenvectors (also known as POD modes or basis functions), $\mathbf{\Sigma}$ is the $N \times M$ diagonal matrix containing the singular values $\{\sigma_1, \sigma_2,...,\sigma_{min(N,M)}\}$ in decreasing order, and $\mathbf{V}^T$ is the transpose of an $M \times M$ matrix containing the right eigenvectors.

If we want to obtain the $L$ basis functions $\mathbf{u_{rb}} = \{\mathbf{u}_1,...,\mathbf{u}_L\}$ that best represent the snapshot matrix $\mathbf{S}$, we would pick the left eigenvectors in $\mathbf{U}$ corresponding to the most significant $L$ singular values from $\mathbf{\Sigma}$ (i.e. $\{\sigma_1, \sigma_2,...,\sigma_{L}\}$). 

We can reinforce two stopping criteria to determine the value of $L$ (i.e., the number of modes that form the reduced basis space). The first one is to set an arbitrary maximum number of modes $N_{max}$ that we are willing to accept. The second criterion is the tolerance on the retained energy $\epsilon$. This is defined as the ratio between the smallest singular value to the largest singular value (i.e. pick $L$ such that $\epsilon \geq \frac{\sigma_{L}}{\sigma_1}$). The smaller $\epsilon$ is, the more information about the full-order model we wish to retain in $\mathbf{V}_{rb}$. In practice, we will stop when either condition is met.

\subsection{Solution Projection}
Once we have the modes, we have formed the reduced basis space $\mathbf{V}_{rb}$ where each FEM solution can then be projected. To project a high-fidelity solution $\mathbf{s}$ onto a reduced basis space represented by $\mathbf{u_{rb}} = \{\mathbf{u}_1,...,\mathbf{u}_L\}$, we can use the Galerkin projection method. This involves computing the inner product of the high-fidelity solution with each basis function and multiplying it by the corresponding basis function. Mathematically, this is given by: 

\begin{equation*}
    \mathbf{s}_{rb} = \sum_{i=1}^{L} \langle \mathbf{s}, \mathbf{u}_i \rangle  \mathbf{u}_i
\end{equation*}

Where the operation $\langle \mathbf{s}, \mathbf{u}_i \rangle$ defines the inner production action between the high-fidelity solution and the $i^{th}$ basis function. This is similar to what we had in equation \ref{eqn: MOR_projection} but for different purposes, where equation \ref{eqn: MOR_projection} was used for estimating the best set of $\mathbf{u}_{rb}$ during the offline stage, and over here we are trying to utilise the information during the online stage.   

\subsection{Solution Reconstruction}

Later in the report, we will discuss the use of ANN in obtaining the predicted solutions on the reduced basis space. However, we are still interested in the full-order solution, which requires a way of reconstructing the high-fidelity solution and measuring the error between the original FEM solution and the reconstructed solution. For a set of basis functions $\mathbf{u_{rb}} = \{\mathbf{u}_1,...,\mathbf{u}_L\}$ and a reduced basis solution $\mathbf{s}_{rb} = \{s_1,...,s_L\}$, the reconstructed solution $\mathbf{v}$ can be simply represented by multiplying the basis functions by their corresponding coefficients and summing over all modes:

\begin{equation}
    \mathbf{v} = \sum_{i=1}^{L} s_i \cdot \mathbf{w}_i 
    \label{eqn:reconstruction}
\end{equation}

To measure the error between the original solution and the reconstructed solution, the relative error was calculated as the ratio of the $L_2$-norm error to the $L_2$-norm of the original solution $\mathbf{u}$, which would give us a percentage that represents the amount of deviation from the FEM solution:

\begin{equation}
   \epsilon = \frac{\| \mathbf{u} - \mathbf{v} \|}{\| \mathbf{u} \|}
   \label{eqn:norm_error}
\end{equation}

The specific norm operation is defined by the specific definition of the inner product action used in the function. The $L_2$-norm is commonly used in functional analysis and finite element methods to measure the magnitude of functions, which is defined in equation \ref{eqn:L2norm}. 

\begin{equation}
    \| f \|_{L_2(\Omega)} = \left( \int_{\Omega} |f(x)| |q(x)| \, dx \right)^{1/2}
    \label{eqn:L2norm}
\end{equation}

The other possible choice is the $H_1$-norm that incorporates the first derivative of the function given by equation \ref{eqn:H1norm}. The $H_1$-norm would incur additional computational cost in computing the gradient term. In this report, we used the $L_2$-norm inner product action for $u$ because it provides a measure of magnitude without considering directional information. 

\begin{equation}
    \| f \|_{H_1(\Omega)} = \left( \| f \|_{L_2(\Omega)}^2 + \| \nabla f \|_{L_2(\Omega)}^2 \right)^{1/2}
    \label{eqn:H1norm}
\end{equation}

For the case of $\sigma$, we included the inner product of the divergence term in the $L_2$-norm. This is similar to equation \ref{eqn:H1norm}, but changed the grad operation to divergence due to $\sigma$ being a vector field, as shown in equation 
\ref{eqn:sigma_norm}. This also helps to capture the spatial variation (i.e. the smoothness) of the fields. 

\begin{equation}
    \| f \| = \left( \| f \|_{L_2(\Omega)}^2 + \| \nabla \cdot f \|_{L_2(\Omega)}^2 \right)^{1/2}
    \label{eqn:sigma_norm}
\end{equation}

\subsection{Choice of Parameters Based on POD Result}
In Section 3, an examination was conducted regarding the selection of parameters ($\bm{\mu}=[\mu_1, \mu_2]$ versus $\bm{\mu} = [\mu_1, \mu_2, \mu_3, \mu_4, \mu_5]$), and we explained our rationale in using only 2 parameters by showing the large variability of the solution manifold imposed by changing 5 parameters altogether. The motivation for choosing 2 parameters is further proved when we examine the POD outcomes obtained using different parametrisations. The choices are checked through a comparison of the POD reconstruction errors. 

Each parameter is adjusted along a Numpy \textit{linspace} scale, ensuring equal intervals between values. Combining each value creates a collection of input parameters. The full-order model was solved for each $\bm{\mu}$, followed by performing POD on the snapshot matrix, which creates a reduced basis space. In the first case, we only varied the two parameters $\mu_1$ and $\mu_2$, each taking 10 values (a total of 100 solutions for POD). In the second case, we varied all 5 parameters taking $4 \times 4 \times 3 \times 3 \times 3$ values (a total of 432 solutions for POD). 

To measure the accuracy and reliability of POD, we initialised 100 random samples of input parameters and solved for the high-fidelity solutions using FEM. The solutions were then projected onto $\mathbf{V}_{rb}$ with $\mathbf{u}_{rb}$ to form the reduced basis solution $\mathbf{s}_{rb}$. The quality of POD can then be quantified by calculating the average relative error of the solution reconstructed using equation \ref{eqn:reconstruction} and \ref{eqn:norm_error}. Getting the POD error as small as possible is an essential first step in the project. Only with a reasonably accurate POD process can we safely proceed to the next step of ANN training, which requires reliable training and validation data. 

In addition to the relative error, we can compare eigenvalues obtained from SVD for different snapshot matrices. If the eigenvalue decays faster, we can obtain a lower intrinsic dimensional space representing the parametric dependence on high-fidelity solutions. This would enhance the computational efficiency and increase the accuracy of POD.

Multiple results have shown that the 5-parameter case would provide a very unstable and inaccurate result in POD, making it harder to proceed with the project. In contrast, changing only 2 parameters would lead to a more desirable POD result. While employing a lot more solution snapshots and modes may make the 5-parameter work slightly better, this was beyond the computational capacity of this project. In addition, POD only creates a linear subspace (i.e. only represents linear combinations of the basis vectors). However, the parameters may interact in complex ways that cannot be adequately represented by linear combinations alone, resulting in a very slow eigenvalue decay rate. These non-linearities of the feature space were particularly relevant for $\mu_3$, $\mu_4$, and $\mu_5$. The detailed results for both the errors and the eigenvalue decay will be presented in Section 6. 

\newpage